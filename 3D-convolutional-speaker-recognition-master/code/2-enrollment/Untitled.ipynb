{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "ename": "UnrecognizedFlagError",
     "evalue": "Unknown command line flag 'f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnrecognizedFlagError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-86053ab5257c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;31m# Load the artificial datasets.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m \u001b[0mfileh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menrollment_dataset_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[0mfileh_development\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevelopment_dataset_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;31m# Train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\sv\\lib\\site-packages\\tensorflow\\python\\platform\\flags.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;31m# a flag.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_parsed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m       \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\sv\\lib\\site-packages\\absl\\flags\\_flagvalues.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, argv, known_only)\u001b[0m\n\u001b[0;32m    631\u001b[0m       \u001b[0msuggestions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_helpers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_flag_suggestions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m       raise _exceptions.UnrecognizedFlagError(\n\u001b[1;32m--> 633\u001b[1;33m           name, value, suggestions=suggestions)\n\u001b[0m\u001b[0;32m    634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_as_parsed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnrecognizedFlagError\u001b[0m: Unknown command line flag 'f'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import tables\n",
    "import numpy as np\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "import random\n",
    "from nets import nets_factory\n",
    "from auxiliary import losses\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "######################\n",
    "# Train Directory #\n",
    "######################\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'test_dir', 'results/TRAIN_CNN_3D/test_logs',\n",
    "    'Directory where checkpoints and event logs are written to.')\n",
    "\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'checkpoint_dir', '../../results/TRAIN_CNN_3D',\n",
    "    'Directory where checkpoints and event logs are written to.')\n",
    "\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'development_dataset_path', '../../data/development_sample_dataset_speaker.hdf5',\n",
    "    'Directory where checkpoints and event logs are written to.')\n",
    "\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'enrollment_dataset_path', '../../data/enrollment-evaluation_sample_dataset.hdf5',\n",
    "    'Directory where checkpoints and event logs are written to.')\n",
    "\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'enrollment_dir', '../../results/Model',\n",
    "    'Directory where checkpoints and event logs are written to.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer('num_clones', 1,\n",
    "                            'Number of model clones to deploy.')\n",
    "\n",
    "tf.app.flags.DEFINE_boolean('clone_on_cpu', False,\n",
    "                            'Use CPUs to deploy clones.')\n",
    "tf.app.flags.DEFINE_boolean('online_pair_selection', False,\n",
    "                            'Use online pair selection.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer('worker_replicas', 1, 'Number of worker replicas.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'num_ps_tasks', 0,\n",
    "    'The number of parameter servers. If the value is 0, then the parameters '\n",
    "    'are handled locally by the worker.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'num_readers', 8,\n",
    "    'The number of parallel readers that read data from the dataset.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'num_preprocessing_threads', 8,\n",
    "    'The number of threads used to create the batches.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'log_every_n_steps', 20,\n",
    "    'The frequency with which logs are print.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'save_summaries_secs', 10,\n",
    "    'The frequency with which summaries are saved, in seconds.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'save_interval_secs', 500,\n",
    "    'The frequency with which the model is saved, in seconds.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'task', 0, 'Task id of the replica running the training.')\n",
    "\n",
    "#######################\n",
    "# Learning Rate Flags #\n",
    "#######################\n",
    "\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'learning_rate_decay_type',\n",
    "    'exponential',\n",
    "    'Specifies how the learning rate is decayed. One of \"fixed\", \"exponential\",'\n",
    "    ' or \"polynomial\"')\n",
    "\n",
    "tf.app.flags.DEFINE_float('learning_rate', 1.0, 'Initial learning rate.')\n",
    "\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'end_learning_rate', 0.0001,\n",
    "    'The minimal end learning rate used by a polynomial decay learning rate.')\n",
    "\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'label_smoothing', 0.0, 'The amount of label smoothing.')\n",
    "\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'learning_rate_decay_factor', 0.94, 'Learning rate decay factor.')\n",
    "\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'num_epochs_per_decay', 2.0,\n",
    "    'Number of epochs after which learning rate decays.')\n",
    "\n",
    "tf.app.flags.DEFINE_bool(\n",
    "    'sync_replicas', False,\n",
    "    'Whether or not to synchronize the replicas during training.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'replicas_to_aggregate', 1,\n",
    "    'The Number of gradients to collect before updating params.')\n",
    "\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'moving_average_decay', None,\n",
    "    'The decay to use for the moving average.'\n",
    "    'If left as None, then moving averages are not used.')\n",
    "\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'model_speech', 'cnn_speech', 'The name of the architecture to train.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'batch_size', 1024, 'The number of samples in each batch.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'num_epochs', 50, 'The number of epochs for training.')\n",
    "\n",
    "# Store all elemnts in FLAG structure!\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "# Load the artificial datasets.\n",
    "fileh = tables.open_file(FLAGS.enrollment_dataset_path, mode='r')\n",
    "fileh_development = tables.open_file(FLAGS.development_dataset_path, mode='r')\n",
    "# Train\n",
    "print(\"Enrollment data shape:\", fileh.root.utterance_enrollment.shape)\n",
    "print(\"Enrollment label shape:\", fileh.root.label_enrollment.shape)\n",
    "\n",
    "# Test\n",
    "print(\"Evaluation data shape:\", fileh.root.utterance_evaluation.shape)\n",
    "print(\"Evaluation label shape:\",fileh.root.label_evaluation.shape)\n",
    "\n",
    "# Get the number of subjects\n",
    "num_subjects_development = len(np.unique(fileh_development.root.label_train[:]))\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    # if not FLAGS.dataset_dir:\n",
    "    #     raise ValueError('You must supply the dataset directory with --dataset_dir')\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "\n",
    "        # required from data\n",
    "        num_samples_per_epoch = fileh.root.label_enrollment.shape[0]\n",
    "        num_batches_per_epoch = int(num_samples_per_epoch / FLAGS.batch_size)\n",
    "\n",
    "        num_samples_per_epoch_test = fileh.root.label_evaluation.shape[0]\n",
    "        num_batches_per_epoch_test = int(num_samples_per_epoch_test / FLAGS.batch_size)\n",
    "\n",
    "        # Create global_step\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "        ######################\n",
    "        # Select the network #\n",
    "        ######################\n",
    "\n",
    "        is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "        # num_subjects is the number of subjects in development phase and not the enrollment.\n",
    "        model_speech_fn = nets_factory.get_network_fn(\n",
    "            FLAGS.model_speech,\n",
    "            num_classes=num_subjects_development,\n",
    "            is_training=is_training)\n",
    "\n",
    "        ##############################################################\n",
    "        # Create a dataset provider that loads data from the dataset #\n",
    "        ##############################################################\n",
    "        # with tf.device(deploy_config.inputs_device()):\n",
    "        \"\"\"\n",
    "        Define the place holders and creating the batch tensor.\n",
    "        \"\"\"\n",
    "        speech = tf.placeholder(tf.float32, (20, 80, 40, 1))\n",
    "        label = tf.placeholder(tf.int32, (1))\n",
    "        batch_dynamic = tf.placeholder(tf.int32, ())\n",
    "        margin_imp_tensor = tf.placeholder(tf.float32, ())\n",
    "\n",
    "        # Create the batch tensors\n",
    "        batch_speech, batch_labels = tf.train.batch(\n",
    "            [speech, label],\n",
    "            batch_size=batch_dynamic,\n",
    "            num_threads=FLAGS.num_preprocessing_threads,\n",
    "            capacity=5 * FLAGS.batch_size)\n",
    "\n",
    "        #############################\n",
    "        # Specify the loss function #\n",
    "        #############################\n",
    "        tower_grads = []\n",
    "        with tf.variable_scope(tf.get_variable_scope()):\n",
    "            for i in xrange(FLAGS.num_clones):\n",
    "                with tf.device('/gpu:%d' % i):\n",
    "                    with tf.name_scope('%s_%d' % ('tower', i)) as scope:\n",
    "                        \"\"\"\n",
    "                        Two distance metric are defined:\n",
    "                           1 - distance_weighted: which is a weighted average of the distance between two structures.\n",
    "                           2 - distance_l2: which is the regular l2-norm of the two networks outputs.\n",
    "                        Place holders\n",
    "\n",
    "                        \"\"\"\n",
    "                        ########################################\n",
    "                        ######## Outputs of two networks #######\n",
    "                        ########################################\n",
    "                        features, logits, end_points_speech = model_speech_fn(batch_speech)\n",
    "\n",
    "\n",
    "                        # one_hot labeling\n",
    "                        # num_subjects is the number of subjects in development phase and not the enrollment.\n",
    "                        # Because we are using the pretrained network in the development phase and use the features of the\n",
    "                        # layer prior to Softmax!\n",
    "                        label_onehot = tf.one_hot(tf.squeeze(batch_labels, [1]), depth=num_subjects_development, axis=-1)\n",
    "\n",
    "                        # Define loss\n",
    "                        with tf.name_scope('loss'):\n",
    "                            loss = tf.reduce_mean(\n",
    "                                tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=label_onehot))\n",
    "\n",
    "                        # Accuracy\n",
    "                        with tf.name_scope('accuracy'):\n",
    "                            # Evaluate the model\n",
    "                            correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(label_onehot, 1))\n",
    "\n",
    "                            # Accuracy calculation\n",
    "                            accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "                            # # ##### call the optimizer ######\n",
    "                            # # # TODO: call optimizer object outside of this gpu environment\n",
    "                            # #\n",
    "                            # # Reuse variables for the next tower.\n",
    "                            # tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "        #################################################\n",
    "        ########### Summary Section #####################\n",
    "        #################################################\n",
    "\n",
    "        # Gather initial summaries.\n",
    "        summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n",
    "\n",
    "        # Add summaries for all end_points.\n",
    "        for end_point in end_points_speech:\n",
    "            x = end_points_speech[end_point]\n",
    "            summaries.add(tf.summary.scalar('sparsity_speech/' + end_point,\n",
    "                                            tf.nn.zero_fraction(x)))\n",
    "\n",
    "    ###########################\n",
    "    ######## ######## #########\n",
    "    ###########################\n",
    "\n",
    "    with tf.Session(graph=graph, config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "\n",
    "        # Initialization of the network.\n",
    "        variables_to_restore = slim.get_variables_to_restore()\n",
    "        saver = tf.train.Saver(variables_to_restore, max_to_keep=20)\n",
    "        coord = tf.train.Coordinator()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "\n",
    "        ################################################\n",
    "        ############## ENROLLMENT Model ################\n",
    "        ################################################\n",
    "\n",
    "        latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir=FLAGS.checkpoint_dir)\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "\n",
    "        # The model predefinition.\n",
    "        NumClasses = 2\n",
    "        NumLogits = 128\n",
    "        MODEL = np.zeros((NumClasses, NumLogits), dtype=np.float32)\n",
    "\n",
    "        # Go through the speakers.\n",
    "        for speaker_id, speaker_class in enumerate(range(1, 3)):\n",
    "\n",
    "            # The contributung number of utterances\n",
    "            NumUtterance = 20\n",
    "            # Get the indexes for each speaker in the enrollment data\n",
    "            speaker_index = np.where(fileh.root.label_enrollment[:] == speaker_class)[0]\n",
    "\n",
    "            # Check the minumum required utterances per speaker.\n",
    "            assert len(speaker_index) >= NumUtterance, \"At least %d utterances is needed for each speaker\" % NumUtterance\n",
    "\n",
    "            # Get the indexes.\n",
    "            start_idx = speaker_index[0]\n",
    "            end_idx = min(speaker_index[0] + NumUtterance, speaker_index[-1])\n",
    "\n",
    "            # print(end_idx-start_idx)\n",
    "\n",
    "            # Enrollment of the speaker with specific number of utterances.\n",
    "            speaker_enrollment, label_enrollment = fileh.root.utterance_enrollment[start_idx:end_idx, :, :,\n",
    "                                                     0:1], fileh.root.label_enrollment[start_idx:end_idx]\n",
    "\n",
    "            # Just adding a dimention for 3D convolutional operations.\n",
    "            speaker_enrollment = speaker_enrollment[None, :, :, :, :]\n",
    "\n",
    "            # Evaluation\n",
    "            feature = sess.run(\n",
    "                [features, is_training],\n",
    "                feed_dict={is_training: True, batch_dynamic: speaker_enrollment.shape[0],\n",
    "                           batch_speech: speaker_enrollment,\n",
    "                           batch_labels: label_enrollment.reshape([label_enrollment.shape[0], 1])})\n",
    "\n",
    "            # Extracting the associated numpy array.\n",
    "            feature_speaker = feature[0]\n",
    "\n",
    "            # # # L2-norm along each utterance vector\n",
    "            # feature_speaker = sklearn.preprocessing.normalize(feature_speaker,norm='l2', axis=1, copy=True, return_norm=False)\n",
    "\n",
    "            # Averaging for creation of the spekear model\n",
    "            speaker_model = feature_speaker\n",
    "\n",
    "            # Creating the speaker model\n",
    "            MODEL[speaker_id,:] = speaker_model\n",
    "\n",
    "        if not os.path.exists(FLAGS.enrollment_dir):\n",
    "            os.makedirs(FLAGS.enrollment_dir)\n",
    "        # Save the created model.\n",
    "        np.save(os.path.join(FLAGS.enrollment_dir , 'MODEL.npy'), MODEL)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sv",
   "language": "python",
   "name": "sv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
